{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3fb7d8",
   "metadata": {},
   "source": [
    "# TASK 1: Rating Prediction via Prompting\n",
    "\n",
    "## Objective\n",
    "This notebook explores how Large Language Models (LLMs) can be used to classify\n",
    "Yelp restaurant reviews into star ratings (1–5) using **prompt engineering**.\n",
    "\n",
    "Instead of training a supervised model, we:\n",
    "- Design multiple prompting strategies\n",
    "- Enforce structured JSON outputs\n",
    "- Evaluate accuracy, JSON validity, and reliability\n",
    "\n",
    "Three different prompt designs are implemented and compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb12e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded44fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We got here around midnight last Friday... the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brought a friend from Louisiana here.  She say...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Every friday, my dad and I eat here. We order ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My husband and I were really, really disappoin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Love this place!  Was in phoenix 3 weeks for w...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  We got here around midnight last Friday... the...      4\n",
       "1  Brought a friend from Louisiana here.  She say...      5\n",
       "2  Every friday, my dad and I eat here. We order ...      3\n",
       "3  My husband and I were really, really disappoin...      1\n",
       "4  Love this place!  Was in phoenix 3 weeks for w...      5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"data/yelp_reviews.csv\")\n",
    "\n",
    "# Keep relevant columns only\n",
    "df = df[[\"text\", \"stars\"]]\n",
    "\n",
    "# Sample ~200 rows\n",
    "df_sample = df.sample(n=200, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df_sample.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4892a52e",
   "metadata": {},
   "source": [
    "## LLM Setup (Free Tier)\n",
    "\n",
    "We use **OpenRouter** with a free, open-source LLM.\n",
    "This is explicitly allowed by the task instructions.\n",
    "\n",
    "Model used:\n",
    "- mistralai/mistral-7b-instruct\n",
    "\n",
    "Temperature is set to 0 to improve consistency for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55481a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17477976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 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\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\")   \n",
    ")\n",
    "\n",
    "MODEL_NAME = \"mistralai/mistral-7b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de79277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a6f07c",
   "metadata": {},
   "source": [
    "## Prompt Version 1 – Baseline\n",
    "\n",
    "A minimal instruction-based prompt.\n",
    "Serves as a baseline to observe raw LLM behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "553b9ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V1 = \"\"\"\n",
    "You are given a Yelp restaurant review.\n",
    "\n",
    "Classify the review into a star rating from 1 to 5.\n",
    "\n",
    "Return the result in the following JSON format:\n",
    "{{\n",
    "  \"predicted_stars\": <integer>,\n",
    "  \"explanation\": \"<short explanation>\"\n",
    "}}\n",
    "\n",
    "Review:\n",
    "\"{review_text}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebd03608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt_v1(review_text: str) -> str:\n",
    "    return call_llm(PROMPT_V1.format(review_text=review_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096525b3",
   "metadata": {},
   "source": [
    "## Prompt Version 2 – Rubric-Based Prompt\n",
    "\n",
    "Introduces an explicit rating rubric to reduce ambiguity,\n",
    "especially between adjacent ratings such as 3 vs 4 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8098566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V2 = \"\"\"\n",
    "You are an expert sentiment analyst.\n",
    "\n",
    "Use the following rubric:\n",
    "1 star: Very negative experience\n",
    "2 stars: Mostly negative with minor positives\n",
    "3 stars: Mixed or neutral experience\n",
    "4 stars: Mostly positive with minor issues\n",
    "5 stars: Extremely positive experience\n",
    "\n",
    "Classify the review strictly using this rubric.\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "  \"predicted_stars\": <1-5>,\n",
    "  \"explanation\": \"<brief explanation>\"\n",
    "}}\n",
    "\n",
    "Review:\n",
    "\"{review_text}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c642b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt_v2(review_text: str) -> str:\n",
    "    return call_llm(PROMPT_V2.format(review_text=review_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d47e0",
   "metadata": {},
   "source": [
    "## Prompt Version 3 – Reasoned Classification + JSON Guard\n",
    "\n",
    "This prompt enforces:\n",
    "- Step-wise reasoning\n",
    "- Strict JSON-only output\n",
    "- Strong constraints on rating values\n",
    "\n",
    "Expected to be the most reliable approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0a98a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V3 = \"\"\"\n",
    "You are a rating prediction system.\n",
    "\n",
    "Step 1: Analyze the sentiment and key points in the review.\n",
    "Step 2: Decide the most appropriate star rating (1–5).\n",
    "Step 3: Output ONLY the final result in valid JSON.\n",
    "\n",
    "Rules:\n",
    "- predicted_stars must be an integer between 1 and 5\n",
    "- Output must be valid JSON\n",
    "- Do not include any extra text\n",
    "\n",
    "JSON format:\n",
    "{{\n",
    "  \"predicted_stars\": <integer>,\n",
    "  \"explanation\": \"<concise explanation>\"\n",
    "}}\n",
    "\n",
    "Review:\n",
    "\"{review_text}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5473c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt_v3(review_text: str) -> str:\n",
    "    return call_llm(PROMPT_V3.format(review_text=review_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f9a5718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(response: str):\n",
    "    try:\n",
    "        return json.loads(response), True\n",
    "    except Exception:\n",
    "        return None, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3937fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation Function \n",
    "def evaluate(prompt_runner, df):\n",
    "    correct = 0\n",
    "    valid_json = 0\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        response = prompt_runner(row[\"text\"])\n",
    "        parsed, is_valid = parse_response(response)\n",
    "\n",
    "        if is_valid:\n",
    "            valid_json += 1\n",
    "            if parsed[\"predicted_stars\"] == row[\"stars\"]:\n",
    "                correct += 1\n",
    "\n",
    "        time.sleep(0.4)  # rate-limit safety\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": correct / len(df),\n",
    "        \"json_validity_rate\": valid_json / len(df)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "633f3cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [06:33<00:00,  1.97s/it]\n",
      "100%|██████████| 200/200 [05:22<00:00,  1.61s/it]\n",
      "100%|██████████| 200/200 [03:40<00:00,  1.10s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'accuracy': 0.06, 'json_validity_rate': 0.12},\n",
       " {'accuracy': 0.01, 'json_validity_rate': 0.02},\n",
       " {'accuracy': 0.015, 'json_validity_rate': 0.015})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_v1 = evaluate(run_prompt_v1, df_sample)\n",
    "results_v2 = evaluate(run_prompt_v2, df_sample)\n",
    "results_v3 = evaluate(run_prompt_v3, df_sample)\n",
    "\n",
    "results_v1, results_v2, results_v3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1b45e",
   "metadata": {},
   "source": [
    "- Prompt V1 shows inconsistent formatting and ambiguous predictions.\n",
    "- Prompt V2 improves accuracy by introducing an explicit rating rubric.\n",
    "- Prompt V3 performs best by combining structured reasoning with strict JSON constraints.\n",
    "\n",
    "This experiment demonstrates that **prompt design alone can significantly improve LLM\n",
    "classification performance** without model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3522e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
